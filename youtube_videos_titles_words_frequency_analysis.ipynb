{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Youtube Video Titles Words Frequency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#needed libraries installation\n",
    "#http requests\n",
    "!pip install requests\n",
    "#natural language processing\n",
    "!pip install nlpk\n",
    "#to remove stop words\n",
    "!pip install stop_words\n",
    "#word cloud graphical representation\n",
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports from built-in and above libraries\n",
    "import requests\n",
    "import json\n",
    "import nltk\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#youtube api credentials\n",
    "api_key = \"<YOUR API KEY>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function used to make youtube api request\n",
    "#if you won't set any region code the defeault is US\n",
    "def fetchYoutubeVideosTitles(keyword, token = '', region_code = 'us'):\n",
    "    local_titles = []\n",
    "    \n",
    "    if len(token)==0:\n",
    "        request_url = \"https://www.googleapis.com/youtube/v3/search?q={}&part=snippet&type=video&maxResults=50&key={}&regionCode={}\".format(search_term, api_key, region_code)\n",
    "    else:\n",
    "        request_url = \"https://www.googleapis.com/youtube/v3/search?q={}&part=snippet&type=video&maxResults=50&key={}&pageToken={}&regionCode={}\".format(search_term, api_key, token, region_code)\n",
    "    \n",
    "    r = requests.get(request_url)\n",
    "    response_content = r.content.decode('utf-8')\n",
    "    \n",
    "    response_json = json.loads(response_content)\n",
    "    \n",
    "    nextPageToken = ''\n",
    "    if 'nextPageToken' in response_json:\n",
    "        nextPageToken = response_json['nextPageToken'] \n",
    "    items = response_json['items']\n",
    "\n",
    "    for item in items:\n",
    "        local_titles.append(item['snippet']['title'])\n",
    "        \n",
    "    return (local_titles, nextPageToken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetch youtube most videos based on the search term below\n",
    "#the code below will put together the paginated results \n",
    "#executing multiple requests if needed (more than 50 results)\n",
    "search_term = \"<YOUR SEARCH TERM>\"\n",
    "print(\"Search Youtube for videos related to {}...\".format(search_term))\n",
    "\n",
    "titles = []\n",
    "\n",
    "page_token = ''\n",
    "api_calls_count = 0\n",
    "while True:\n",
    "    print(\"Search Youtube for videos related to {}... Iteration {}\".format(search_term, api_calls_count))\n",
    "    fetch_function_results = fetchYoutubeVideosTitles(search_term, page_token)\n",
    "    #print(fetch_function_results)\n",
    "    \n",
    "    titles.extend(fetch_function_results[0])\n",
    "    page_token = fetch_function_results[1]\n",
    "    print(\"Search Youtube for videos related to {}... Next Page Token = {}.\".format(search_term, page_token) )\n",
    "    \n",
    "    if page_token == '':\n",
    "        break\n",
    "        \n",
    "    api_calls_count += 1\n",
    "    \n",
    "print(\"Search Youtube for videos related to {}. DONE.\".format(search_term))\n",
    "\n",
    "print(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare titles list for words frequency analysis\n",
    "titles_words_joined = \" \".join(titles)\n",
    "titles_words_joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('gutenberg')\n",
    "nltk.download('genesis')\n",
    "nltk.download('inaugural')\n",
    "nltk.download('nps_chat')\n",
    "nltk.download('webtext')\n",
    "nltk.download('treebank')\n",
    "nltk.download('punkt')\n",
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use this function to filter out numbers from the text to analyze\n",
    "#from: https://www.pythoncentral.io/how-to-check-if-a-string-is-a-number-in-python-including-unicode/\n",
    "def is_number(s):\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        pass\n",
    " \n",
    "    try:\n",
    "        import unicodedata\n",
    "        unicodedata.numeric(s)\n",
    "        return True\n",
    "    except (TypeError, ValueError):\n",
    "        pass\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#frequency analysis (many lines below from https://onlinecoursetutorials.com/nlp/how-to-remove-punctuation-in-python-nltk/)\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "words = tokenizer.tokenize(titles_words_joined)\n",
    "\n",
    "#remove stop words\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from stop_words import get_stop_words\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = list(get_stop_words('en')) #put here the right country iso code for the titles language    \n",
    "nltk_words = list(stopwords.words('english')) #put here the right language \n",
    "\n",
    "#in my stop words put, if needed otherwise leave empty, the words not filtered\n",
    "#using the two lines above that you want to filter out\n",
    "my_stop_words = ['any_word_you_want_to_filter_out']\n",
    "stop_words.extend(nltk_words)\n",
    "stop_words.extend(my_stop_words)\n",
    "\n",
    "#the len(w)>2 can be removed from the conditions below if you want to do frequency analysis of \n",
    "#those so short words(not filtered out as stop words) if any\n",
    "output = [w for w in words if (not w in stop_words and not is_number(w) and len(w)>2)] \n",
    "\n",
    "freqDist = FreqDist(output)\n",
    "freqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot words frequency of the youtube video titles\n",
    "freqDist.plot(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#draw word cloud graph (from https://stackoverflow.com/questions/16645799/how-to-create-a-word-cloud-from-a-corpus-in-python)\n",
    "def show_wordcloud(data, title = None):\n",
    "    wordcloud = WordCloud(\n",
    "        background_color='white',\n",
    "        max_words=200,\n",
    "        max_font_size=40, \n",
    "        scale=3,\n",
    "        random_state=1 # chosen at random by flipping a coin; it was heads\n",
    "    ).generate(str(data))\n",
    "\n",
    "    fig = plt.figure(1, figsize=(12, 12))\n",
    "    plt.axis('off')\n",
    "    if title: \n",
    "        fig.suptitle(title, fontsize=20)\n",
    "        fig.subplots_adjust(top=2.3)\n",
    "\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.show()\n",
    "    \n",
    "show_wordcloud(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
